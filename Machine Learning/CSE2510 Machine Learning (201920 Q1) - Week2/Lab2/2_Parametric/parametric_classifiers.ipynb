{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Parametric Classifiers\n",
    "Machine Learning 2019/2020 <br>\n",
    "Ruben Wiersma and David Tax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHAT** This nonmandatory lab consists of several programming and insight exercises/questions on Gaussian density estimation.\n",
    "\n",
    "**WHY** The exercises are meant to familiarize yourself with the basic concepts of parametric classifiers.\n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a friend. If you want to skip right to questions and exercises, find the $\\rightarrow$ symbol. Use [Mattermost][1] to discuss questions with your peers. For additional questions and feedback please consult the TA's during the lab session. \n",
    "\n",
    "[1]: https://mattermost.ewi.tudelft.nl/ml/channels/town-square\n",
    "$\\newcommand{\\q}[1]{\\rightarrow \\textbf{Question #1}}$\n",
    "$\\newcommand{\\ex}[1]{\\rightarrow \\textbf{Exercise #1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bayes classifier\n",
    "\n",
    "In this assignment, you will implement your own bayes classifier. Because this is your first assignment, we will walk you through the steps from loading and inspecting the data, to running your classifier.\n",
    "\n",
    "Specifically, this assignment consists of the following steps:\n",
    "0. Classification using Gaussian distributions\n",
    "1. Getting to know the data\n",
    "2. Validation sets\n",
    "3. Univariate model\n",
    "4. Probability density function\n",
    "5. Posterior probabilities\n",
    "6. Bayes classifier\n",
    "\n",
    "Work your way through these exercises at your own pace and be sure to ask questions to the TA's when you don't understand something. It's important that you get what is happening here, as it is a fundamental building block of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Classification using Gaussian distributions\n",
    "\n",
    "We are starting with a very important notion in machine learning: probability distributions. Occurrences of data typically follow probability distributions that we know how to model.\n",
    "\n",
    "Say, you want to classify apples vs. oranges. A _feature_ that you could use to classify them is their colour. We know, of course, that oranges are orange and apples (the golden delicious kind) are green, but each orange is a slightly different shade of orange. Likewise, the apples are all a different shade of green. If we would plot the colour values against the number of fruits with that colour, we would see, however, that there are probably more oranges with a certain type of shade than with other colours. They tend to follow known probability distributions.\n",
    "\n",
    "In this assignment, we will assume data that has a normal distribution and try to estimate the parameters of the normal distribution to correctly fit our data, hence the name _parametric_ classifiers. We will then use Bayes' rule to build a classifier based on the probability distribution.\n",
    "\n",
    "Just to refresh your mind, this is what a normal distribution looks like:\n",
    "![Normal distribution for oranges](gaussian.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of apples and oranges, we will try to classify flowers from Fisher's Iris dataset. The dataset contains the measurements of *length* and *width* of the *sepals* and *petals* of 150 flowers. \n",
    "\n",
    "![Petal and sepal in Iris flowers](Petal-sepal.jpg)\n",
    "\n",
    "Using these 4 attributes (*length* and *width* of both), the flowers should then be classified as one of 3 species of Iris flower:\n",
    "\n",
    "* Iris setosa\n",
    "* Iris versicolor\n",
    "* Iris virginica\n",
    "\n",
    "This dataset is such a classic example that is even included in machine learning libraries. The following code will load the dataset from `scikit-learn` (this was installed with conda) into the variable `iris`.\n",
    "\n",
    "$\\ex{0.1}$ Run the code and inspect what data is contained in `iris`. Can you identify the 4 attributes? What other information is contained in `iris`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting to know the data\n",
    "\n",
    "The dataset is stored as a dictionary, a data structure in Python that resembles a hashmap. We can access items in the dictionary with a dot `.`, so we access the data and their target labels with `iris.data` and `iris.target`. If we want to know what each digit means, we can access the names with `iris.target_names`.\n",
    "\n",
    "$\\ex{1.1}$ Run the code fragment and confirm what it is doing. Try to understand the indexing and print the following data:\n",
    "- The last five flowers. Expected result: a [5 x 4] array.\n",
    "- Only the third feature of each flower. Expected result: a [150 x 1] array.\n",
    "- The names of the first ten flowers. Expected result: a [10 x 1] array with strings.\n",
    "- Three separate arrays (one for each class). Expected result: three [50 x 4] arrays. Try doing this without assuming anything about the indices for each class, i.e.: do not simply use `class1 = iris.data[:50, :]`\n",
    "\n",
    "__Hint__ Look at the indexing chapter in last week's lab for help.\n",
    "\n",
    "__Hint__ For the final exercise, you can use `np.where`. This function takes in a boolean statement and returns the indices for which the statement is true. Example use: `np.where(iris.target == 0)` returns all indices where the target label is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First five flowers: \\n\", iris.data[:5, :])\n",
    "print(\"Their labels: \", iris.target[:5])\n",
    "print(\"And the label names: \", iris.target_names)\n",
    "\n",
    "# STUDENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can make plots of our data to see how it is distributed.\n",
    "\n",
    "$\\ex{1.2}$ Run the following code to plot the petal length and width of each flower as a scatterplot. Inspect the code carefully, as you will need to write your own code for plotting later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the Matplotlib library, import pyplot. We will refer to this library later as plt.\n",
    "# This is a package for python that lets you create images and plot your data.\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Create a scatterplot of the first two features, and use their labels as colour values.\n",
    "plt.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)\n",
    "plt.xlabel('Sepal length (first feature)')\n",
    "plt.ylabel('Sepal width (second feature)')\n",
    "plt.show()\n",
    "# Create a scatterplot of the third and fourth feature.\n",
    "plt.scatter(iris.data[:, 2], iris.data[:, 3], c=iris.target)\n",
    "plt.xlabel('Petal length (third feature)')\n",
    "plt.ylabel('Petal width (fourth feature)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{1.1}$ How are the points distributed? Could you fit a probability distribution that you know on this data (e.g. uniform, normal, etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Validation sets\n",
    "\n",
    "Now that we have an idea what our dataset looks like, our goal is to create a model that will predict the class of each flower based on its attributes. In order to evaluate how well the model fits, we will also need a validation set where we can test some of our predictions. For this, we will split the data randomly in a train and validation set.\n",
    "\n",
    "$\\ex{2.1}$ Use the code below to split the dataset into a train and validation set.\n",
    "\n",
    "__Tip__ we use the function `train_test_split`. It is easy to confuse the test set with the validation set, but it is important to separate the two: you can use your validation set when you are creating and adjusting your model. In the contrary, the test set is only to be used after you are done adjusting your model. If you change your model to better fit the test set, you risk overfitting on the data that you have. In other words: you perform well on the data you know, but might do really bad on data you don't know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #to split in train and test set\n",
    "\n",
    "# load the data and create the training and test sets\n",
    "iris = datasets.load_iris()\n",
    "# X is the feature vectors for the data points, and y is the target (ground truth) class for those data points \n",
    "#  the iris.data and iris.target entries are randomly divided into training and validation sets.\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(iris.data, iris.target, test_size=0.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Univariate model\n",
    "\n",
    "Looking at the plots of the data from the previous section, you might assume that separating the different classes would be a lot easier based on the petal data (3rd and 4th variable) than on the sepal data (1st and 2nd variable), as it is easier to distinguish the different clusters in that plot. In fact, for now we will only focus on one variable, the petal length (3rd variable), as it looks like it might be useful just on its own and this will simplify the model a lot.\n",
    "\n",
    "Let's first take a look at the distribution of flowers along this variable to confirm that our assumption of a normal distribution is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(iris.data[:50, 2], label=iris.target_names[0])\n",
    "plt.hist(iris.data[50:100, 2], label=iris.target_names[1])\n",
    "plt.hist(iris.data[100:150, 2], label=iris.target_names[2])\n",
    "plt.xlabel('Petal length')\n",
    "plt.ylabel('Number of flowers')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks about correct! Now, let's find the parameters of the normal distribution that describe our data best. The parameters that we need to find are the mean and standard deviation. \n",
    "\n",
    "$\\ex{3.1}$ Using the training data from each of 3 classes, compute the mean ($\\mu$) and standard deviation ($\\sigma$) for the *pedal length* attribute. The Maximum Likelihood Estimators for these are given by\n",
    "\n",
    "(3.1) $$\\mu = \\frac{\\sum_{t=1}^Nx^t}{N}$$\n",
    "\n",
    "(3.2) $$\\sigma = \\sqrt{\\frac{\\sum_{t=1}^N(x^t - m)^2}{N}}$$\n",
    "\n",
    "__Hint__ Try to use numpy's functions to perform operations on your input (e.g. `np.sum`, `np.sqrt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean(x):\n",
    "    mean = 0\n",
    "    # STUDENT\n",
    "    \n",
    "def compute_sd(x, mean):\n",
    "    sd = 0\n",
    "    # STUDENT\n",
    "\n",
    "# Separate the dataset into the three flower types.\n",
    "x_0 = X_train[np.where(y_train == 0)]\n",
    "x_1 = X_train[np.where(y_train == 1)]\n",
    "x_2 = X_train[np.where(y_train == 2)]\n",
    "\n",
    "# Compute the mean for each flower type.\n",
    "mean_0 = compute_mean(x_0[:, 2])\n",
    "mean_1 = compute_mean(x_1[:, 2])\n",
    "mean_2 = compute_mean(x_2[:, 2])\n",
    "\n",
    "# Compute the standard deviation for each flower type.\n",
    "sd_0 = compute_sd(x_0[:, 2], mean_0)\n",
    "sd_1 = compute_sd(x_1[:, 2], mean_1)\n",
    "sd_2 = compute_sd(x_2[:, 2], mean_2)\n",
    "\n",
    "# Print the computed means and standard deviations.\n",
    "print(mean_0, mean_1, mean_2)\n",
    "print(sd_0, sd_1, sd_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{3.1}$ Do these mean values and standard deviations correspond to the histograms that we plotted? If not, try to fix your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Probability density function\n",
    "\n",
    "The probability density function for a Gaussian distribution is defined as\n",
    "\n",
    "(4.1) $$p(x|\\mu, \\sigma)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}$$\n",
    "\n",
    "where $X$ is Gaussian (normal) distributed with mean $\\mu$ and variance $\\sigma^2$, denoted as $\\mathcal{N}(\\mu, \n",
    "\\sigma^2$).\n",
    "\n",
    "That means that if we have estimates for $\\mu$ and $\\sigma$, we can compute the probability density for a specific value $x$.\n",
    "\n",
    "$\\ex{4.1}$ Implement the `normal_PDF` function below. Given `x`, `mean`, and `sd`, we want to return the result of $p(x|\\mu, \\sigma)$. Your PDF is plotted. Play around with different configurations of `mean` and `sd` to see how these parameters influence your normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def normal_PDF(x, mean, sd):\n",
    "    pdf = 0\n",
    "    # STUDENT\n",
    "\n",
    "# Set x, mean and standard deviation\n",
    "x = 0.5\n",
    "mean = 2\n",
    "sd = 0.5\n",
    "my_pdf = normal_PDF(x, mean, sd)\n",
    "\n",
    "# You can compare your outcome to scipy's built-in normal PDF\n",
    "scipy_pdf = norm.pdf(x, mean, sd)\n",
    "print(\"Your pdf function outcome: \", my_pdf, \" Scipy's function outcome: \", scipy_pdf)\n",
    "\n",
    "# And we plot the result of your PDF function for 100 points between 0 and 4: np.linspace(0, 4, 100)\n",
    "xs = np.linspace(0, 4, 100)\n",
    "plt.plot(xs, normal_PDF(xs, mean, sd))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already made estimates for $\\mu$ and $\\sigma$ for the *petal length* for each of the 3 classes, so we can now define PDFs for each separate class.\n",
    "\n",
    "$\\ex{4.2}$ Plot the 3 functions using [linspace](https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.linspace.html) for a range of x-values aside the histograms of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of the flower types\n",
    "plt.hist(iris.data[:50, 2], label=iris.target_names[0])\n",
    "plt.hist(iris.data[50:100, 2], label=iris.target_names[1])\n",
    "plt.hist(iris.data[100:150, 2], label=iris.target_names[2])\n",
    "\n",
    "# Plot your PDFs here, using mean_{0..2}, sd_{0..2}\n",
    "xs = np.linspace(0, 7, 100)\n",
    "plt.plot(xs, normal_PDF(xs, mean_0, sd_0), c='k')\n",
    "plt.plot(xs, normal_PDF(xs, mean_1, sd_1), c='k')\n",
    "plt.plot(xs, normal_PDF(xs, mean_2, sd_2), c='k')\n",
    "\n",
    "plt.xlabel('Petal length')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{4.1}$ Do your distributions and the histogram overlap? In what ways are the histogram and the probability distributions different?\n",
    "\n",
    "__Hint__ The histogram shows the number of flowers that have a petal length within a certain window (bin). That means the values shown in the histogram are absolute counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Posterior probabilities\n",
    "\n",
    "The plot above shows the probability densities for a value $x$. For a normal distribution of a class, you only need to know the mean and the standard deviation to be able to determine the probability that a data point $x$ is from a class provided $C_i$, i.e. $p(x | \\mu_i, \\sigma_i)$ ; e.g. the probability that an unknown fruit is an apple given the apples' class distribution. This is equivalent to the probability density given that specific class, i.e. $p(x | C_i)$.\n",
    "\n",
    "$\\q{5.1}$ Stop for a moment to try and understand what this probability means: $p(x | C_i)$.\n",
    "\n",
    "__Hint__ The $|$ sign in $p(x | C_i)$ means: given that.\n",
    "\n",
    "\n",
    "However, what would be useful for classification, is the posterior probabilities of the classes given the data, i.e. $P(C_i | x)$.\n",
    "\n",
    "$\\q{5.2}$ Can you explain this mathematical formulation in your own words?\n",
    "\n",
    "$\\q{5.3}$ Why is it helpful to know the posterior probability?\n",
    "\n",
    "__Hint__ What information do we have for test points coming in?\n",
    "\n",
    "\n",
    "To get the posterior probability, we can use Bayes' rule:\n",
    "\n",
    "(5.1) $$P(C_i | x) =  \\frac{p(x | C_i) P(C_i)}{p(x)} = \\frac{p(x | C_i) P(C_i)}{\\sum_{k=1}^K p(x | C_k) P(C_k)}$$\n",
    "\n",
    "Because here we have no prior knowledge of the distribution of the different classes, we can just assume all prior class probabilities $P(C_i)$ to be equal. For our 3 class problem, that would mean a probability of $\\frac{1}{3}$ for each class, but we can also just factor the common prior out of the equation and simplify to\n",
    "\n",
    "$$P(C_i | x) = \\frac{p(x | C_i)}{\\sum_{k=1}^K p(x | C_k)}$$\n",
    "\n",
    "\n",
    "$\\ex{5.1}$ Finish the code to compute the posterior probability of a point $x$, given the mean, standard deviation, and class index.\n",
    "\n",
    "__Tip__ The mean and standard deviation are given as arrays. You can access the mean for class `i` with `mean[i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def posterior(x, means, sds, i):\n",
    "    \"\"\"\n",
    "    Compute the posterior probabiliy P(C_i | x).\n",
    "    :param x: the sample to compute the posterior probability for.\n",
    "    :param means: an array of means for each class.\n",
    "    :param sds: an array of standard deviation values for each class.\n",
    "    :param i: the index of the class to compute the posterior probability for.\n",
    "    \"\"\"\n",
    "    # First we compute the probability density function for class i\n",
    "    # STUDENT\n",
    "    \n",
    "    # Next, we compute the sum of pdfs and use this to calculate the posterior probability\n",
    "    # STUDENT\n",
    "\n",
    "means = [mean_0, mean_1, mean_2]\n",
    "sds = [sd_0, sd_1, sd_2]\n",
    "\n",
    "# Test out the code\n",
    "i = 2\n",
    "flower_id = 2\n",
    "x_post = posterior(X_train[flower_id, 2], means, sds, i) \n",
    "\n",
    "print(\"Posterior probability for class\", iris.target_names[i], \": \", x_post)\n",
    "print(\"Flower belongs to class\", iris.target_names[y_train[flower_id]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{5.2}$ Plot the posterior probabilities for all 3 classes. Does the plot of these 3 posteriors make sense based on the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(0, 7, 100)\n",
    "plt.plot(xs, posterior(xs, means, sds, 0), label=iris.target_names[0])\n",
    "plt.plot(xs, posterior(xs, means, sds, 1), label=iris.target_names[1])\n",
    "plt.plot(xs, posterior(xs, means, sds, 2), label=iris.target_names[2])\n",
    "plt.xlabel('Petal length')\n",
    "plt.ylabel('Posterior probability')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{5.4}$ Where would you put the decision boundary for each class? In other words: where would you draw the line, separating each class. Could you formulate this mathematically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bayes Classifier\n",
    "\n",
    "Now that we can compute the posteriors for every class, constructing a classifier is easy. The Bayes classifier is defined as\n",
    "\n",
    "- Classify as $C_i$ for which: $i = argmax_i\\ P(C_i |x)$\n",
    "\n",
    "$\\ex{6.1}$ Write the code for the `classify` function. It should classify a single data point $x$ as one of the 3 classes, returning $0$, $1$ or $2$ based on the class the flower is most likely to belong to. The other arguments of the function should therefore be the vector of mean estimates `means` and the vector of standard deviation estimates `sds`, where index `i` corresponds to class $C_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(x, means, sds):\n",
    "    # STUDENT\n",
    "\n",
    "# Test out the code\n",
    "flower_id = 8\n",
    "predicted_class = classify(X_train[flower_id, 2], means, sds) \n",
    "\n",
    "print(\"Predicted class\", iris.target_names[predicted_class])\n",
    "print(\"Flower belongs to class\", iris.target_names[y_train[flower_id]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{6.2}$ Finally, complete the `validate` function below. It should take a validation set, the expected class (ground truth; correct classifications of each element in the validation set) for all data points in that set and the vectors `means` and `sds` with which to calculate the classifications (decides which class each point belongs to) based on the distributions learnt from the training set. The function should return the percentage of elements in the validation set that were classified correctly.\n",
    "\n",
    "__Hint__ You will only need to use the *petal length* variable from each data point to attempt to classify it (since that is how we trained our model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(X_validation, target, means, sds):\n",
    "    preds = []\n",
    "    # STUDENT\n",
    "\n",
    "validate(X_validation[:, 2], y_validation, means, sds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to our scatterplots and see how your classifier makes decisions. For this, we also plot the decision boundary. The function to create the decision boundaries does this in a very simple way:\n",
    "- For each class, compute the posterior for 1000 points between 1 and 7\n",
    "- If for any two classes the posteriors are equal at a point, add that point to the list of decision boundaries\n",
    "- Plot vertical lines at these points\n",
    "\n",
    "$\\q{6.1}$ This method is quite complicated. Could you analytically solve the equations? __Hint__ You have to find the $x$ for which the class probabilities are equal. You can formulate this equatlity with the probability functions and solve that equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_boundary(means, sds):\n",
    "    decision_boundaries = []\n",
    "    for x in np.linspace(1, 7, 1000):\n",
    "        posteriors = []\n",
    "        for i in range(3):\n",
    "            posteriors.append(posterior(x, means, sds, i))\n",
    "        if ((np.abs(posteriors[0] - posteriors[1]) < 1e-4 and posteriors[0] > 1e-5 and posteriors[1] > 1e-5) or \n",
    "            (np.abs(posteriors[0] - posteriors[2]) < 1e-4 and posteriors[0] > 1e-5 and posteriors[2] > 1e-5) or\n",
    "            (np.abs(posteriors[1] - posteriors[2]) < 1e-2 and posteriors[1] > 1e-5 and posteriors[2] > 1e-5)):\n",
    "            decision_boundaries.append(x)\n",
    "    return decision_boundaries\n",
    "\n",
    "# Create a scatterplot of the third and fourth feature.\n",
    "plt.scatter(iris.data[:, 2], iris.data[:, 3], c=iris.target)\n",
    "plt.xlabel('Petal length (third feature)')\n",
    "plt.ylabel('Petal width (fourth feature)')\n",
    "decision_boundaries = decision_boundary(means, sds)\n",
    "for boundary in decision_boundaries:\n",
    "    plt.axvline(x=boundary)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
