{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Unsupervised Learning\n",
    "Machine Learning 2019/2020 <br>\n",
    "Ruben Wiersma and Gosia Migut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHAT** This nonmandatory lab consists of several programming and insight exercises/questions on unsupervised learning with k-means clustering and PCA. \n",
    "\n",
    "**WHY** The exercises are meant to familiarize yourself with the basic concepts of unsupervised learning.\n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a friend. There is quite a bit of theory and explanation in these notebooks. If you want to skip right to questions and exercises, find the $\\rightarrow$ symbol. Use [Mattermost][1] to discuss questions with your peers. For additional questions and feedback please consult the TA's during the lab session. \n",
    "\n",
    "[1]: https://mattermost.ewi.tudelft.nl/ml/channels/town-square\n",
    "$\\newcommand{\\q}[1]{\\rightarrow \\textbf{Question #1}}$\n",
    "$\\newcommand{\\ex}[1]{\\rightarrow \\textbf{Exercise #1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning without examples\n",
    "\n",
    "As a child you probably taught yourself how to separate building blocks from each other. Even without someone _telling you_ which shape was which, you could decide which shapes belonged to each other to make separate heaps of cubes, cylinders, and pyramids. You performed a form of _unsupervised learning_: clustering.\n",
    "\n",
    "In the previous part, you applied data dimensionality techniques. Now will apply these techniques to a new dataset and perform k-means clustering on the retrieved features.\n",
    "\n",
    "### Structure\n",
    "\n",
    "This assignment consists of two parts:\n",
    "- In [Part 1](part1_dimensionalityreduction.ipynb), you will get familiar with dimensionality reduction using PCA.\n",
    "- In [Part 2](part2_clustering.ipynb), you will apply your PCA algorithm and practice with k-Means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1a: Preparing the data\n",
    "\n",
    "In this assignment, you will work with the TIDIGITS dataset. This dataset was created by Texas Instruments (hence, TI) and is a set of voice recordings of digits. You can go ahead and give the audio files a listen in the data folder: `data/tidigits/...`.\n",
    "\n",
    "Before we can use any clustering algorithms on these data, we need a way to describe our audio files in a way that a computer can understand. An audio file is read as an array, where each value in the array is the amplitude of the audio at a the corresponding time. We refer to these data as _raw_ waveforms. It's infeasible to use the raw data for clustering, therefore we need to extract a limited number of _features_ to describe each audiofile: __feature extraction__.\n",
    "\n",
    "We will extract _MFCC_ features. To extract these, we split the audio file in frames of 25ms each. Next, we apply a number of complicated operations to retrieve $13$ features per frame. If a file is split in, for example, 100 frames, this means we have $13 * 100 = 1300$ features! To bring this number down, we sample  5  frames from regular intervals (the size of the interval is dependent on the length of the audio file) and flatten this to an array of  5âˆ—13=65  features (this number is thus independent of the length of the audio file).\n",
    "\n",
    "$\\ex{1a.1}$ Using pip (Python's package manager), install `python_speech_features` and `soundfile`:\n",
    "```\n",
    "$ pip install python_speech_features SoundFile\n",
    "```\n",
    "If all this fails, we have a backup dataset for you down below.\n",
    "\n",
    "$\\ex{1a.2}$ Run the code below to extract MFCC features for the 150 files provided. This will give you a dataset with 50 'one' audiofiles, 50 'two' audiofiles and 50 'three' audiofiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import python_speech_features as features\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def extract_mfcc(sound, sample = 5):\n",
    "    # Read in the flac file\n",
    "    data, samplerate = sf.read(sound)\n",
    "    \n",
    "    # Extract MFCC features.\n",
    "    # For each frame (25ms segment of audio) in the file, we get 13 MFCC features,\n",
    "    # giving us a [n x 5] matrix of features\n",
    "    mfcc_feat = np.asarray(features.mfcc(data,samplerate), dtype='float32')\n",
    "    \n",
    "    # We sample 5 frames and flatten the feature vectors into one large 'supervector'.\n",
    "    idx = np.floor(np.linspace(0, mfcc_feat.shape[0] - 1, sample)).astype(int)\n",
    "    mfcc_sampled = mfcc_feat[idx]\n",
    "    mfcc_feat_vector = mfcc_sampled.flatten()\n",
    "    \n",
    "    return mfcc_feat_vector\n",
    "\n",
    "# Read audiofiles and extract MFCC feature vectors\n",
    "one = []\n",
    "for i in range(50):\n",
    "    feat = extract_mfcc(\"data/tidigits/1/{:d}.flac\".format(i))\n",
    "    one.append(feat)\n",
    "two = []\n",
    "for i in range(50):\n",
    "    feat = extract_mfcc(\"data/tidigits/2/{:d}.flac\".format(i))\n",
    "    two.append(feat)\n",
    "three = []\n",
    "for i in range(50):\n",
    "    feat = extract_mfcc(\"data/tidigits/3/{:d}.flac\".format(i))\n",
    "    three.append(feat)\n",
    "\n",
    "# Concatenate into one large dataset\n",
    "X_train = np.concatenate((one, two, three))\n",
    "ones = np.ones(50)\n",
    "y_train = np.concatenate((ones, ones * 2, ones * 3)).astype(int)\n",
    "np.savetxt(\"data/tidigits_features.txt\", X_train)\n",
    "np.savetxt(\"data/tidigits_targets.txt\", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup if you cannot load the MFCC code\n",
    "X_train = np.loadtxt(\"data/tidigits_features.txt\")\n",
    "y_train = np.loadtxt(\"data/tidigits_targets.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the number of features\n",
    "\n",
    "Now we have 150 data points (i.e., 150 audio files) and each data point consists of 65 features describing the audio file. Let's plot the the first two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well... it seems like we can't cluster our points based on the first two features. There must be some combination of features that will allow us to cluster our points. If only there was a way to automatically find a combination of features that will allow us to cluster the data points, without supervision...\n",
    "\n",
    "But wait! We do have such a technique: PCA. And you've just implemented it in the previous part. Let's go ahead and apply PCA to find the four most descriptive 'directions' in the data and project the features onto these directions.\n",
    "\n",
    "This involves the following steps:\n",
    "- First compute the covariance matrix of the dataset\n",
    "- Compute the first four eigenvectors of the covariance matrix\n",
    "- Project the feature vector for point in the original dataset on these four eigenvectors\n",
    "    * Projecting a vector onto another vector is done with the dot product\n",
    "   \n",
    "This should give us four features for each point that describe our audio well. We are plotting just two of these features, as this can be easily visualised, but will need four features to accurately cluster all points.\n",
    "\n",
    "$\\ex{1a.3}$ Complete the following code to reduce the number of features with PCA. Plot your results.\n",
    "\n",
    "__Hint__ you can use your own implementation of PCA, or use PCA from Scipy's or Numpy's linear algebra library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First compute the covariance matrix of the dataset\n",
    "\n",
    "# STUDENT\n",
    "\n",
    "# END STUDENT\n",
    "\n",
    "# Next, retrieve the four eigenvectors of the covariance matrix\n",
    "# CHALLENGE: you can use your own implementation of power iteration to find the eigenvectors.\n",
    "\n",
    "# STUDENT\n",
    "\n",
    "# END STUDENT\n",
    "\n",
    "# We project our points onto the eigenvectors using matrix multiplication\n",
    "X_train_reduced = np.zeros((X_train.shape[0], 4))\n",
    "for i in range(len(X_train)):\n",
    "    X_train_reduced[i, :] = np.matmul(X_train[i, :].T, eigenvectors).T\n",
    "    \n",
    "# And plot the points with the new data\n",
    "plt.scatter(X_train_reduced[:, 0], X_train_reduced[:, 1], c=y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have ourselves a dataset that we can start clustering!\n",
    "\n",
    "__Important note__ We have now shown the labels for each point as the colour, but we will not be using the labels for training. You'll just use the labels to verify the clusters your are deriving from the data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: K-Means clustering\n",
    "\n",
    "First, we will implement the (original) k-means clustering algorithm. This algorithm works as follows: we start with $k$ clusters and pick some random centre (mean) for each cluster. Next, we assign points to each cluster based on the distance to each centre and we update the centre to the mean of all points in the cluster. This is repeated until the centres stop moving.\n",
    "\n",
    "Pointwise, the steps of the k-means clustering algorithm approach are as follows:\n",
    "1. Initialize $k$ cluster centers at random locations.\n",
    "2. Assign each point to a cluster.\n",
    "3. Update the cluster centers.\n",
    "4. Go to step 2 unless converged or a number of iterations have been done.\n",
    "\n",
    "First, we create a `Cluster` class.\n",
    "\n",
    "$\\ex{1.1}$ Finish the method that calculates the centroid (other word for centre) of the cluster,\n",
    "`Cluster.centroid`. The centroid is the mean vector of all feature vectors. This method will get called frequently, while the cluster (and thus the centroid) sometimes remains unchanged. To increase efficiency, store and reuse the cluster centroid until the cluster itself changes (i.e. points are added or removed), at which point a new centroid should be calculated.\n",
    "\n",
    "__Hint__ use the changed flag to see if the centroid needs to be recomputed, or if the current value is still valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This Object is used to store the clusters. A Cluster object consists of a numpy \n",
    "# matrix containing all feature vectors in the cluster and the centroid of all the vectors.\n",
    "# The object also contains a boolean for speedup purposes.\n",
    "class Cluster(object):\n",
    "\n",
    "    def __init__(self, array=np.matrix([])):\n",
    "        self.changed = True\n",
    "        self.data = np.matrix(array)\n",
    "        self.cd = self.data\n",
    "        \n",
    "    def reset_cluster(self):\n",
    "        self.data = np.array([])\n",
    "    \n",
    "    def is_changed(self):\n",
    "        return self.changed\n",
    "    \n",
    "    def set_changed(self, changed):\n",
    "        self.changed = changed\n",
    "    \n",
    "    def set_centroid(self, vector):\n",
    "        self.cd = vector\n",
    "    \n",
    "    def append(self, other):\n",
    "        # Set changed flag to true (the cluster has changed)\n",
    "        self.set_changed(True)\n",
    "        \n",
    "        self.data = np.vstack((self.data, other))\n",
    "\n",
    "    def centroid(self):\n",
    "        #If the matrix consists of 1 vector, no need to compute centroid.\n",
    "        if len(np.shape(self.data)) == 1:\n",
    "            return self.data\n",
    "        # Checking whether the cluster has changed since last computation (for speedup)\n",
    "        if self.is_changed():\n",
    "            ## Missing code here => implement yourself\n",
    "# -------------------------------Student ---------------------------------------        \n",
    "            \n",
    "# -------------------------------end Student ---------------------------------------         \n",
    "        # Set changed flag to false, untill it changes.\n",
    "        self.set_changed(False)\n",
    "        return self.cd\n",
    "    \n",
    "# Some small test cases the Cluster class\n",
    "c = Cluster(np.array([[0, 1], [2, 0]]))\n",
    "# Verifies that the centroid is calculated correctly\n",
    "np.testing.assert_array_equal(c.centroid(), np.array([[1.0, 0.5]]))\n",
    "\n",
    "# Verifies that the centroid is calculated correctly after a new data point has been added.\n",
    "c.append(np.array([1, 2]))\n",
    "np.testing.assert_array_equal(c.centroid(), np.array([[1.0, 1.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's take a look at the data that we will be clustering. Note we plotted the data here as the algorithm will see it, thus without labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# This function can be used to load the given cluster data into a list of cluster objects\n",
    "# It is also allowed to create your own function to load the data\n",
    "def read_file(file):\n",
    "    lines = [line.rstrip('\\n') for line in open(file)]\n",
    "    # init data structure\n",
    "    points = []\n",
    "    for line in lines:\n",
    "        # Take FeatureVector from dataset\n",
    "        elements = line.split(\" \")\n",
    "        p = [float(el) for el in elements]\n",
    "        # Append FeatureVector to the list of clusters\n",
    "        points.append(p)\n",
    "    points = np.asarray(points)\n",
    "    return points\n",
    "\n",
    "# If the previous steps for feature extraction and reduction failed, you can use these data.\n",
    "# Otherwise, we will use the data and features you extracted yourself!\n",
    "# points = read_file(\"data/cluster.txt\")\n",
    "points = X_train_reduced\n",
    "x = points[:, 0]\n",
    "y = points[:, 1]\n",
    "plt.scatter(x, y, c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Now you will start implementing k-Means. Let's start at step 1: Initialize k cluster centers at random locations.\n",
    "\n",
    "$\\ex{1.2}$  Finish the `addInitPoints` function. This function selects a random point in the dataset, constructs a cluster around it and adds the cluster to the list of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "# This function selects random k points from the dataset , for each random point it initializes the cluster \n",
    "# and adds the cluster to the list of clusters.\n",
    "def add_init_points(points, clusters, k):\n",
    "# -------------------------------Student ---------------------------------------\n",
    "\n",
    "# -------------------------------end Student ---------------------------------------\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Now you will implement the update step. In the update process, we assign points to clusters based on their distance to each centroid.\n",
    "\n",
    "$\\ex{1.3}$ Implement the `distance` function. We will use Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(p1, p2):\n",
    "    # Euclidian distance between 2 points (in any space)\n",
    "# -------------------------------Student ---------------------------------------\n",
    "\n",
    "# -------------------------------end Student ---------------------------------------\n",
    "\n",
    "# Verifies that the distance metric is correct\n",
    "np.testing.assert_array_equal(distance(np.zeros([1, 2]), np.ones([1, 2])), np.sqrt(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{1.4}$ Implement the update step of the k-Means clustering algorithm. First, use `addInitPoint` if no clusters are present yet to initialize $k$ clusters. After this check, we perform an iteration of the k-Means clustering algorithm. This consists of the following steps: \n",
    "1. Calculate the centroids of each cluster and save them.\n",
    "2. Then remove all points from all clusters.\n",
    "3. Finally add each point to the closest cluster centroid (the centroids that you saved earlier).\n",
    "\n",
    "__Hint__ You will want to save the cluster centroids, because otherwise the cluster centroids will change during step 3 because they will be recalculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# This function updates the the kClusters list with a given amount of k clusters for \n",
    "# each feature vector from the clusters list\n",
    "def update_k_means(points, clusters, k):\n",
    "    # Reset Clusters   \n",
    "    centroids = []\n",
    " \n",
    "    # Add initial points\n",
    "    # If add_clusters is true, initalise clusters with add_init_points\n",
    "    # Then add the cluster centroids to the centroids list\n",
    "    add_clusters = len(clusters) == 0\n",
    "# 1-------------------------------Student ---------------------------------------   \n",
    "    \n",
    "\n",
    "    \n",
    "# 1-------------------------------end Student --------------------------------------- \n",
    "    # Reset clusters from last iteration,\n",
    "    # so the clustering can be performed with new centroids.\n",
    "    for cluster in clusters:\n",
    "        cluster.reset_cluster()\n",
    "        \n",
    "    clusters = [None for el in range(k)]\n",
    "    for p in points:\n",
    "    \n",
    "    # Calculate the min distance to one of the centroids\n",
    "# 2-------------------------------Student ---------------------------------------\n",
    "\n",
    "# 2-------------------------------end Student ---------------------------------------\n",
    "    # Add the data point to the cluster with min_distance to centroid\n",
    "        if label >= 0:\n",
    "            if clusters[label] is None:\n",
    "                clusters[label] = Cluster(p)\n",
    "            else:\n",
    "                clusters[label].append(p)\n",
    "         \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "$\\ex{1.5a}$ Use the `plot_k_means_data` function to plot the clustered data (you might have to run your algorithm with a different random seed to get a proper clustering) and have it analyse the TIDIGITS dataset (or `data/cluster.txt`) with $k$ set to 3. Run the k-means clustering algorithm and verify that it is clustering the points as intended. Use `plot_k_means_data_update` to see the cluster updates at each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "    \n",
    "def plot_k_means_data( clusters, k, itr):\n",
    "    colors = cm.brg(np.linspace(0,1,k))\n",
    "    for (i,cl) in enumerate(colors):\n",
    "        x = [[el[0,0]] for el in clusters[i].data]\n",
    "        y = [[el[0,1]] for el in clusters[i].data] \n",
    "        plt.scatter(x, y, c=[cl])\n",
    "        plt.scatter(clusters[i].centroid()[0,0], clusters[i].centroid()[0,1], c='black')\n",
    "        plt.title(\"Clusters at update \" + str(itr))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_k_means_data_update(clusters_prev, clusters, k):\n",
    "    if(clusters_prev == []):\n",
    "        return\n",
    "    colors = cm.brg(np.linspace(0,1,k))\n",
    "    for (i,cl) in enumerate(colors):\n",
    "        \n",
    "        x = [[el[0,0]] for el in clusters[i].data]\n",
    "        y = [[el[0,1]] for el in clusters[i].data] \n",
    "        plt.scatter(x, y, c=[cl], marker = '*')\n",
    "\n",
    "        plt.scatter(clusters[i].centroid()[0,0], clusters[i].centroid()[0,1], c='black')\n",
    "        \n",
    "        x_prev = [[el[0,0]] for el in clusters_prev[i].data]\n",
    "        y_prev = [[el[0,1]] for el in clusters_prev[i].data]\n",
    "        \n",
    "        plt.scatter(x_prev, y_prev, c=[cl], alpha=0.2, s=100)\n",
    "        \n",
    "        plt.scatter(clusters_prev[i].centroid()[0,0], clusters_prev[i].centroid()[0,1], c='green')\n",
    "        plt.legend(['New clusters', 'New centroids', 'Previous clusters', 'Previous centroids'], loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        \n",
    "        plt.title('Difference between updates')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "random.seed(42)\n",
    "points = X_train_reduced # Ex 1.5a\n",
    "# points = X_train # Ex 1.5b (optional)\n",
    "clusters = []\n",
    "clusters_prev = []\n",
    "centroids = []\n",
    "k = 3\n",
    "centroids_prev = [np.zeros((points.shape[1])) for x in range(0,k)]\n",
    "# Run here the updateKMeans function with k=3 using the kClusters list. If the initial centroids \n",
    "# are off you might have to run it a few times to update them accordingly!\n",
    "# -------------------------------Student ---------------------------------------\n",
    "# Running it 10 times to be sure the centroids are updated right. if there is no change in centroids in consecutive\n",
    "# iterations, stop updating\n",
    "\n",
    "\n",
    "# -------------------------------end Student ---------------------------------------\n",
    "# Ground truth\n",
    "plt.scatter(points[:, 0], points[:, 1], c=y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\ex{1.5b}$ (Optional) Try using the clustering on just raw audiofiles (`X_train`). How does this compare to the PCA-extracted features with regards to accuracy and speed? For accuracy, take a look at `sklearn.metrics.homogeneity_score` and `sklearn.metrics.completeness_score`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{1.1}$ Compare the clusters that were computed using k-means with the actual labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "\n",
    "Now that we have a cluster, we would like to find out how good our clustering is. We will also do this in an unsupervised approach: without knowing the correct labels for each cluster, we _can_ say something about the spread of each cluster.\n",
    "\n",
    "For this purpose you will compute the _sum of residual squares_ of the cluster. This is computed as follows: sum over the squared euclidean distances between each point and their corresponding centroid and divide by the total number of points. In math:\n",
    "\n",
    "$$\n",
    "    srs = \\frac{1}{N} \\sum_{i \\in C} (p_i - c)^2\n",
    "$$\n",
    "\n",
    "Where $N$ is the number of points, $C$ is the cluster containing points $i$ and centroid $c$ and $p_i$ is the feature vector for point $i$.\n",
    "\n",
    "$\\ex{1.6}$ Implement the `calculate_average_sum_rs` function, which computes the average sum of residual squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the average sum of residual squares of the given cluster\n",
    "def calculate_average_sum_rs(cluster):\n",
    "    if len(cluster.data) == 0:\n",
    "        return None\n",
    "# -------------------------------Student ---------------------------------------    \n",
    "\n",
    "# -------------------------------end Student ---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "\n",
    "Next, we will use this metric to try and automatically determine how many clusters we should use.\n",
    "\n",
    "$\\ex{1.7}$ Implement the `tune_k` method. This method should test out several values for $k$. Let $k$ range from 1 to 15. Then, for each $k$, run the k-means algorithm on `data/cluster.txt` with $10$ update iterations. After the algorithm is done, calculate the average sum RS of all clusters and print them to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "# This function tries a t_k number of k's for the updateKMeans function (our kMean classifier)\n",
    "# and calculates the SRS for each k running the updateKMean tk times for each k.\n",
    "def tune_k(min_k, max_k, n_updates):\n",
    "    assert 0 < min_k < max_k\n",
    "    assert n_updates > 1\n",
    "    srss = []\n",
    "    # -------------------------------Student --------------------------------------- \n",
    "\n",
    "    # -------------------------------end Student --------------------------------------- \n",
    "    plt.plot(list(range(len(srss))), srss, marker='o')\n",
    "    plt.grid(linestyle='-', linewidth=1)\n",
    "    plt.show()\n",
    "    \n",
    "min_k=1\n",
    "max_k=15\n",
    "n_updates=10\n",
    "tune_k(min_k, max_k, n_updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{1.2}$ What seems to be a good choice for $k$, judging by the results? Note: a lower SRS means each cluster is more compact. The optimal value of K can be selected by considering the \"elbow method\".\n",
    "\n",
    "$\\q{1.3}$ What would be the average SRS if we set $k$ equal to the number of points in our dataset?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
