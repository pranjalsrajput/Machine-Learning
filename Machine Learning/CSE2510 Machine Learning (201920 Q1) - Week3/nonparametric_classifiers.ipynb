{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Non-parametric Classifiers\n",
    "Machine Learning 2019/2020 <br>\n",
    "Ruben Wiersma and Gosia Migut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHAT** This nonmandatory lab consists of several programming and insight exercises/questions on k-nn density estimation.\n",
    "\n",
    "**WHY** The exercises are meant to familiarize yourself with the basic concepts of non-parametric classifiers.\n",
    "\n",
    "**HOW** Follow the exercises in this notebook either on your own or with a friend. If you want to skip right to questions and exercises, find the $\\rightarrow$ symbol. Use [Mattermost][1] to discuss questions with your peers. For additional questions and feedback please consult the TA's during the lab session. \n",
    "\n",
    "[1]: https://mattermost.ewi.tudelft.nl/ml/channels/qa-week-3\n",
    "$\\newcommand{\\q}[1]{\\rightarrow \\textbf{Question #1}}$\n",
    "$\\newcommand{\\ex}[1]{\\rightarrow \\textbf{Exercise #1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbours\n",
    "\n",
    "Last week, you got acquainted with parametric classifiers. You described a distribution using only a few parameters (mean and standard deviation) and tried to find the values for those parameters that best fit the data. This week, you will work on non-parametric classifiers. As the name implies, these classifiers do not use parameters to describe their data. Instead, they directly use training data in the classification process, or set up rules to classify new samples.\n",
    "\n",
    "A popular example of non-parametric classifiers is the K-Nearest Neighbours (K-NN) classifier. In this exercise, you will find out how it works by implementing it yourself and you'll get to know in what circumstances to use it. In this assignment, we will walk you through the following steps in the K-NN algorithm:\n",
    "\n",
    "1. Load data: Open the dataset from CSV and split into test/train datasets.\n",
    "2. Similarity: Calculate the distance between two data instances.\n",
    "3. Nearest Neighbors: Locate k most similar data instances.\n",
    "4. Majority vote: Get the neighbours to vote on the class of the test points.\n",
    "5. Accuracy: Summarize the accuracy of predictions.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load data\n",
    "In this notebook we will work with the Iris dataset.\n",
    "First we import all the modules that you need for this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets # to load the dataset\n",
    "from sklearn.model_selection import train_test_split #to split in train and test set\n",
    "from sklearn.model_selection import cross_val_score #BONUS\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from scipy.spatial import distance\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data and create the training and test sets\n",
    "iris = datasets.load_iris()\n",
    "# X is the feature vectors for the data points, and y is the target (ground truth) class for those data points \n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{1.1}$ Print and plot the data to understand what you will be classifying. You can plot points using Matplotlib's [scatter](https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.scatter.html) function. We have already imported `pyplot` as `plt`. This means you can call the `scatter` function with `plt.scatter(x, y, ...)`.\n",
    "\n",
    "__Hint__ each sample has four features, you can just plot two of them to get a basic idea of the data.<br>\n",
    "__Hint__ use the `c=y_train` parameter to colour each point with its class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n",
    "plt.xlabel('Sepal length (first feature)')\n",
    "plt.ylabel('Sepal width (second feature)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{1.1}$ Would it work to classify this dataset with a parametric classifier? Why? Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Similarity\n",
    "\n",
    "Next, we will create a function to compute distance between two points $ \\mathbf{p}, \\mathbf{q} $. We will use the euclidean distance, a distance function that is often used.\n",
    "\n",
    "$\\ex{2.1}$ Complete the `euclidean` function. This function should compute the euclidean distance between two points:\n",
    "\n",
    "$$\n",
    "d(\\mathbf{p}, \\mathbf{q}) = \\sqrt{(\\mathbf{p} - \\mathbf{q})\\cdot(\\mathbf{p} - \\mathbf{q})}\n",
    "$$\n",
    "\n",
    "__Hint__ You might know a more specific formulation of this as $|\\mathbf{p}| = \\sqrt{p_1^2 + p_2^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(p, q):\n",
    "    \"\"\"\n",
    "    Computes the euclidean distance between point p and q.\n",
    "    :param p: point p as a numpy array.\n",
    "    :param q: point q as a numpy array.\n",
    "    :return: distance as float.\n",
    "    \"\"\"\n",
    "    \n",
    "    # STUDENT\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{2.1}$ Could you name a few other distance functions? What would be the effect of choosing another distance function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Nearest Neighbours\n",
    "\n",
    "Now that we can define a distance between points, we will try to find the $k$ (e.g. 5) nearest neighbours in the training set for a test instance.\n",
    "\n",
    "$\\q{3.1}$ Given $n$ training samples and $m$ test instances, express the number of steps (complexity) this would take in big-O notation: $O(...)$.\n",
    "\n",
    "$\\ex{3.1}$ Complete the `get_neighbours` function.\n",
    "\n",
    "__Challenge__ If you are limited by storage to $O(k)$, what datastructure would you use to store the $k$ nearest neighbours? Can you implement this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbours(training_set, test_instance, k):\n",
    "    \"\"\"\n",
    "    Calculate distances from test_instance to all training points.\n",
    "    :param training_set: [n x d] numpy array of training samples (n: number of samples, d: number of dimensions).\n",
    "    :param test_instance: [d x 1] numpy array of test instance features.\n",
    "    :param k: number of neighbours to return.\n",
    "    :return: list of length k with neighbour indices.\n",
    "    \"\"\"\n",
    "    \n",
    "    distances = []\n",
    "    \n",
    "    for i, training_instance in enumerate(training_set):\n",
    "        # Compute the distance to each item in the training set\n",
    "        # STUDENT\n",
    "        \n",
    "    \n",
    "    # Return only k closest neighbours\n",
    "    # STUDENT\n",
    "    \n",
    "    \n",
    "    return neighbours\n",
    "\n",
    "neighbours = get_neighbours(X_train, X_test[0], 5)\n",
    "neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that our implementation is correct by plotting the points in 2D.\n",
    "\n",
    "$\\ex{3.2}$ Use the provided plot code to show the nearest neighbours for a couple of different values for $k$ and a number of test samples. Is your function working?\n",
    "\n",
    "__Tip__ remember that the dataset contains four features, while only two features are plotted. This could explain anomalies that show up.\n",
    "\n",
    "__Tip__ the larger datapoints are the k-NNs of the test point. Note that these do not have to be of the same color (even though here they are)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_neighbours(X_train, y_train, test_instance, k):\n",
    "    \"\"\"\n",
    "    Plots all points in the dataset and shows the neighbours of a given test point.\n",
    "    \"\"\"\n",
    "    \n",
    "    neighbours = get_neighbours(X_train, test_instance, k)\n",
    "    # initialization of the sizes of the points to be plotted, size 10 \n",
    "    neigh_sizes = np.ones((len(y_train), 1)) * 10\n",
    "    neigh_sizes[neighbours] = 50\n",
    "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=neigh_sizes)\n",
    "    plt.scatter(test_instance[0], test_instance[1], c='r', s=50, marker='x')\n",
    "    plt.show()\n",
    "\n",
    "for i in range(3):\n",
    "    test_instance = X_test[i]\n",
    "    k = 5\n",
    "    plt.xlabel('Sepal length (first feature)')\n",
    "    plt.ylabel('Sepal width (second feature)')\n",
    "    plot_neighbours(X_train, y_train, test_instance, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Majority vote\n",
    "\n",
    "We have the $k$ nearest neighbours of the test set. Now we will choose a label by majority vote.\n",
    "\n",
    "$\\ex{4.1}$ Implement the `get_majority_vote` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter #to count unique occurances of items in array, for majority voting\n",
    "\n",
    "def get_majority_vote(neighbours, training_labels):\n",
    "    \"\"\"\n",
    "    Given an array of nearest neighbours indices for a given test case, \n",
    "    tally up their classes to vote on the correct class for the test instance.\n",
    "    :param neighbours: list of nearest neighbour indices.\n",
    "    :param training_labels: the list of labels for each training instance.\n",
    "    :return: the label of most common class.\n",
    "    \"\"\"\n",
    "    # STUDENT\n",
    "    \n",
    " \n",
    "predicted_label = get_majority_vote(neighbours, y_train)\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Accuracy\n",
    "\n",
    "Now we will put all the code you wrote above together and try to summarise the accuracy of this method.\n",
    "\n",
    "$\\ex{5.1}$ Complete the `predict` function below and compute the accuracy on `X_test, y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_train, X_test, y_train, y_test, k=5):\n",
    "    \"\"\"\n",
    "    Predicts all labels for the test set, using k-nn on the training set and computes the accuracy.\n",
    "    :param X_train: the training set features.\n",
    "    :param X_test: the test set features.\n",
    "    :param y_train: the training set labels.\n",
    "    :param y_test: the test set labels.\n",
    "    :return: list of predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # generate predictions\n",
    "    predictions = []\n",
    "    \n",
    "    # for each instance in the test set, get nearest neighbours and majority vote on predicted class\n",
    "    # STUDENT\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# load the data and create the training and test sets\n",
    "iris = datasets.load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4)\n",
    "k = 5\n",
    "\n",
    "predictions = predict(X_train, X_test, y_train, y_test, k)\n",
    "\n",
    "# summarize performance of the classification\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('The overall accuracy of the model is: {:f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{5.2}$ Complete the `accuracy_score_self` and use this to compute your own accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_self(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of a test set as the fraction of items that was classified correctly.\n",
    "    :param y_test: the list of true labels for the test set.\n",
    "    :param y_pred: the list of predicted labels for the test set.\n",
    "    :return: accuracy as a floating point.\n",
    "    \"\"\"\n",
    "    \n",
    "    # STUDENT\n",
    "    \n",
    "\n",
    "# summarize performance of the classification\n",
    "accuracy_self = accuracy_score_self(y_test, predictions)\n",
    "\n",
    "print('Accuracy using scikit-learn: ', accuracy)\n",
    "print('Accuracy using own accuracy: ', accuracy_self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{5.3}$ Complete the `plot_errors` function to get a better understanding of why some points are misclassified. You can use the `plot_neighbours` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors(X_train, X_test, y_train, y_test, predictions, k):\n",
    "    \"\"\"\n",
    "    Plots the test points that were misclassified and their nearest neighbours using plot_neighbours.\n",
    "    \"\"\"\n",
    "    \n",
    "    # STUDENT\n",
    "    \n",
    "    \n",
    "plot_errors(X_train, X_test, y_train, y_test, predictions, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ex{5.4}$ Test out several values of $k$ to find the best $k$. Automate this process, e.g. for $k = 1...10$ compute the average accuracy over 10 repetitions (to average over randomness in train/test splits; i.e. [cross validation](https://machinelearningmastery.com/k-fold-cross-validation/)) and plot the accuracy for each $k$.\n",
    "\n",
    "__Hint__ You can store all the results in a `[10 x n_iterations]` numpy array and `np.mean` with `axis=1` to compute the mean accross the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 10\n",
    "accuracies = np.zeros((10, n_iterations))\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4)\n",
    "    for k in range(1, 11):        \n",
    "    # STUDENT\n",
    "        \n",
    "\n",
    "plt.plot(range(1, 11), accuracies)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{5.2}$ What $k$ would you pick, based on your results? Does it matter a lot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are plotting to learn about our classifier, let's take a brief look at learning curves. For a learning curve, we plot the number of samples (x-axis) in the train set against the accuracy (y-axis).\n",
    "\n",
    "$\\q{5.3}$ What would you expect the learning curve to look like for the k-NN classifier?\n",
    "\n",
    "Let's go ahead and create a learning curve.\n",
    "\n",
    "$\\ex{5.5}$ Read through the code to understand what is happening and execute to plot the learning curve. Try this for several values for $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 7\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4)\n",
    "# Set up array to store accuracies\n",
    "accuracies = np.zeros(X_train.shape[0])\n",
    "\n",
    "# We want to learn with at least k samples and up to the size of the train set\n",
    "for i in range(k, X_train.shape[0]):\n",
    "    predictions = predict(X_train[:i], X_test, y_train[:i], y_test, k)\n",
    "    accuracies[i] = accuracy_score_self(y_test, predictions)\n",
    "    \n",
    "# Plot learning curve\n",
    "plt.plot(range(X_train.shape[0]), accuracies)\n",
    "plt.xlabel('Number of training samples')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\q{5.4}$ Did the learning curve resemble the expected curve? If not: why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
